{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 9794060,
          "sourceType": "datasetVersion",
          "datasetId": 6001791
        },
        {
          "sourceId": 9873051,
          "sourceType": "datasetVersion",
          "datasetId": 6061018
        }
      ],
      "dockerImageVersionId": 30787,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "V11iyBZ0kBzP"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "swethas658768767_datasetone_path = kagglehub.dataset_download('swethas658768767/datasetone')\n",
        "swethas658768767_newkeypoints_path = kagglehub.dataset_download('swethas658768767/newkeypoints')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "s0ENc64nkBzS"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pip install mediapipe"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-20T15:12:17.949077Z",
          "iopub.execute_input": "2024-11-20T15:12:17.949579Z",
          "iopub.status.idle": "2024-11-20T15:12:49.534257Z",
          "shell.execute_reply.started": "2024-11-20T15:12:17.949528Z",
          "shell.execute_reply": "2024-11-20T15:12:49.532781Z"
        },
        "id": "rCMiPcV_kBzT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pip install opencv-python"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-20T15:23:40.039639Z",
          "iopub.execute_input": "2024-11-20T15:23:40.040741Z",
          "iopub.status.idle": "2024-11-20T15:23:50.174892Z",
          "shell.execute_reply.started": "2024-11-20T15:23:40.040624Z",
          "shell.execute_reply": "2024-11-20T15:23:50.173666Z"
        },
        "id": "TbwINdPokBzT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pip install SpeechRecognition"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-21T16:01:55.277726Z",
          "iopub.execute_input": "2024-11-21T16:01:55.278079Z",
          "iopub.status.idle": "2024-11-21T16:02:05.233334Z",
          "shell.execute_reply.started": "2024-11-21T16:01:55.278049Z",
          "shell.execute_reply": "2024-11-21T16:02:05.232252Z"
        },
        "id": "8t4teoFukBzT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "def zip_folder(folder_path, output_path):\n",
        "    \"\"\"Zips a folder and its contents.\n",
        "\n",
        "    Args:\n",
        "        folder_path: The path to the folder to zip.\n",
        "        output_path: The path to the output zip file.\n",
        "    \"\"\"\n",
        "    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, _, files in os.walk(folder_path):\n",
        "            for file in files:\n",
        "                zipf.write(os.path.join(root, file),\n",
        "                           os.path.relpath(os.path.join(root, file),\n",
        "                                           os.path.join(folder_path, '..')))\n",
        "\n",
        "# Example usage:\n",
        "folder_to_zip = '/kaggle/working/keypoints'  # Replace with your folder path\n",
        "output_zip_file = '/kaggle/working/keypoints.zip'  # Provide the full path including file name\n",
        "\n",
        "zip_folder(folder_to_zip, output_zip_file)\n",
        "print(f\"Folder '{folder_to_zip}' zipped to '{output_zip_file}'\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-20T15:13:10.860146Z",
          "iopub.execute_input": "2024-11-20T15:13:10.860567Z",
          "iopub.status.idle": "2024-11-20T15:13:10.871016Z",
          "shell.execute_reply.started": "2024-11-20T15:13:10.860531Z",
          "shell.execute_reply": "2024-11-20T15:13:10.86971Z"
        },
        "id": "Hdk0X0oDkBzT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from mediapipe import solutions\n",
        "\n",
        "# Initialize Mediapipe Holistic\n",
        "mp_holistic = solutions.holistic\n",
        "holistic = mp_holistic.Holistic(static_image_mode=False, model_complexity=2)\n",
        "\n",
        "# Path to new dataset directory\n",
        "dataset_path = '/kaggle/input/datasetone/videosone'\n",
        "output_path = '/kaggle/working/keypoints/'  # Directory to save keypoint .npy files\n",
        "os.makedirs(output_path, exist_ok=True)     # Ensure output directory exists\n",
        "\n",
        "# Target length to pad sequences to (e.g., 30 frames)\n",
        "target_length = 30\n",
        "\n",
        "# Function to extract keypoints from a frame\n",
        "def extract_keypoints(frame):\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    results = holistic.process(frame_rgb)\n",
        "\n",
        "    # Extract features: face, left and right hands, and pose\n",
        "    keypoints = []\n",
        "\n",
        "    # Face\n",
        "    if results.face_landmarks:\n",
        "        keypoints += [np.array([lm.x, lm.y, lm.z]) for lm in results.face_landmarks.landmark]\n",
        "    else:\n",
        "        keypoints += [np.zeros(3)] * 468\n",
        "\n",
        "    # Left Hand\n",
        "    if results.left_hand_landmarks:\n",
        "        keypoints += [np.array([lm.x, lm.y, lm.z]) for lm in results.left_hand_landmarks.landmark]\n",
        "    else:\n",
        "        keypoints += [np.zeros(3)] * 21\n",
        "\n",
        "    # Right Hand\n",
        "    if results.right_hand_landmarks:\n",
        "        keypoints += [np.array([lm.x, lm.y, lm.z]) for lm in results.right_hand_landmarks.landmark]\n",
        "    else:\n",
        "        keypoints += [np.zeros(3)] * 21\n",
        "\n",
        "    # Pose\n",
        "    if results.pose_landmarks:\n",
        "        keypoints += [np.array([lm.x, lm.y, lm.z]) for lm in results.pose_landmarks.landmark]\n",
        "    else:\n",
        "        keypoints += [np.zeros(3)] * 33\n",
        "\n",
        "    return np.array(keypoints).flatten()  # Flatten to a 1D array\n",
        "\n",
        "# Function to process each video and save its features as a .npy file\n",
        "def process_videos():\n",
        "    for video_file in os.listdir(dataset_path):\n",
        "        video_path = os.path.join(dataset_path, video_file)\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        frames = []\n",
        "\n",
        "        # Read each frame\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            keypoints = extract_keypoints(frame)\n",
        "            frames.append(keypoints)\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "        # Check if frames list is empty\n",
        "        if frames:\n",
        "            # Pad or truncate frames to the target length\n",
        "            if len(frames) < target_length:\n",
        "                frames += [np.zeros_like(frames[0])] * (target_length - len(frames))\n",
        "            frames = np.array(frames[:target_length])  # Truncate if longer\n",
        "\n",
        "            # Save features for the current video\n",
        "            video_name = os.path.splitext(video_file)[0]  # Remove file extension\n",
        "            npy_save_path = os.path.join(output_path, f\"{video_name}_features.npy\")\n",
        "            np.save(npy_save_path, frames)\n",
        "            print(f'Saved features for video {video_file} at {npy_save_path}')\n",
        "        else:\n",
        "            print(f\"Warning: No frames found in video {video_file}\")\n",
        "\n",
        "# Run the feature extraction\n",
        "process_videos()\n",
        "holistic.close()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-20T15:13:19.996488Z",
          "iopub.execute_input": "2024-11-20T15:13:19.996896Z",
          "iopub.status.idle": "2024-11-20T15:13:23.058097Z",
          "shell.execute_reply.started": "2024-11-20T15:13:19.996858Z",
          "shell.execute_reply": "2024-11-20T15:13:23.056219Z"
        },
        "id": "WOgVg8u-kBzU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow.keras import layers, Input, Model, Sequential\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle  # For saving and loading the tokenizer\n",
        "\n",
        "# Paths to data folders and save path\n",
        "keypoints_folder = '/kaggle/input/newkeypoints/keypoints/keypoints'\n",
        "metadata_path = '/kaggle/input/newkeypoints/metadata_with_transcriptions.csv'\n",
        "save_path = '/kaggle/working/'\n",
        "\n",
        "# GAN Parameters\n",
        "latent_dim = 100\n",
        "sequence_length = 30\n",
        "num_keypoints = 543 * 3\n",
        "batch_size = 8\n",
        "epochs = 350\n",
        "\n",
        "# Load metadata and perform train-test split\n",
        "metadata = pd.read_csv(metadata_path)\n",
        "train_metadata, test_metadata = train_test_split(metadata, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and fit the Tokenizer on the training metadata if needed\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")  # Adjust num_words to fit vocabulary size\n",
        "tokenizer.fit_on_texts(train_metadata['phrase'])  # Assuming 'transcription' column holds text data\n",
        "\n",
        "# Save the tokenizer to use in deployment\n",
        "with open(os.path.join(save_path, 'tokenizer.pickle'), 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# Load keypoints function\n",
        "def load_keypoints(data_folder, metadata):\n",
        "    features = []\n",
        "    for _, row in metadata.iterrows():\n",
        "        file_path = os.path.join(data_folder, row['filename'].replace(\".mp4\", \"_features.npy\"))\n",
        "        if os.path.exists(file_path):\n",
        "            feature = np.load(file_path)\n",
        "            if feature.shape == (sequence_length, num_keypoints):\n",
        "                features.append(feature)\n",
        "            else:\n",
        "                print(f\"Warning: Skipping {row['filename']} due to shape mismatch {feature.shape}\")\n",
        "        else:\n",
        "            print(f\"Warning: {file_path} not found.\")\n",
        "    return np.array(features)\n",
        "\n",
        "# Load datasets\n",
        "train_features = load_keypoints(keypoints_folder, train_metadata)\n",
        "test_features = load_keypoints(keypoints_folder, test_metadata)\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_features).shuffle(buffer_size=1000).batch(batch_size)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(test_features).batch(batch_size)\n",
        "\n",
        "# Text-to-Feature Mapping Model\n",
        "def build_text_to_feature_mapping():\n",
        "    text_input = Input(shape=(None,), dtype=\"int32\")  # Updated to accept integer sequences\n",
        "    x = layers.Embedding(input_dim=5000, output_dim=latent_dim)(text_input)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    latent_vector = layers.Dense(latent_dim)(x)\n",
        "    return Model(text_input, latent_vector, name=\"text_to_feature_mapping\")\n",
        "\n",
        "# Generator Model\n",
        "def build_generator():\n",
        "    input_noise = Input(shape=(latent_dim,))\n",
        "    x = layers.Dense(sequence_length * num_keypoints, activation=\"relu\")(input_noise)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Reshape((sequence_length, num_keypoints))(x)\n",
        "    x = layers.Bidirectional(layers.LSTM(512, return_sequences=True, activation=\"tanh\"))(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = layers.Bidirectional(layers.LSTM(256, return_sequences=True, activation=\"relu\"))(x)\n",
        "    attention_output = layers.MultiHeadAttention(num_heads=4, key_dim=128)(x, x)\n",
        "    x = layers.Add()([x, attention_output])\n",
        "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, activation=\"relu\"))(x)\n",
        "    x = layers.LSTM(256, return_sequences=True, activation=\"tanh\")(x)\n",
        "    output = layers.TimeDistributed(layers.Dense(num_keypoints, activation=\"tanh\"))(x)\n",
        "    return Model(input_noise, output)\n",
        "\n",
        "# Discriminator Model\n",
        "def build_discriminator():\n",
        "    model = Sequential([\n",
        "        layers.Input(shape=(sequence_length, num_keypoints)),\n",
        "        layers.GaussianNoise(0.1),\n",
        "        layers.Conv1D(64, kernel_size=3, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(0.2),\n",
        "        layers.Conv1D(128, kernel_size=3, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(0.2),\n",
        "        layers.Conv1D(256, kernel_size=3, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(0.2),\n",
        "        layers.Dropout(0.4),\n",
        "        layers.Conv1D(512, kernel_size=3, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(0.2),\n",
        "        layers.Conv1D(256, kernel_size=3, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(0.2),\n",
        "        layers.Dropout(0.4),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Instantiate models\n",
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "text_to_feature_mapping = build_text_to_feature_mapping()\n",
        "\n",
        "# Losses and Optimizers\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy()\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5)\n",
        "\n",
        "\n",
        "# Training code remains the same as before, and can be executed to pre-train the models\n",
        "# Training Step Function\n",
        "@tf.function\n",
        "def train_step(video_batch):\n",
        "    noise = tf.random.normal([batch_size, latent_dim])\n",
        "    with tf.GradientTape() as disc_tape, tf.GradientTape() as gen_tape:\n",
        "        generated_videos = generator(noise, training=True)\n",
        "\n",
        "        real_output = discriminator(video_batch, training=True)\n",
        "        fake_output = discriminator(generated_videos, training=True)\n",
        "\n",
        "        disc_loss = cross_entropy(tf.ones_like(real_output), real_output) + \\\n",
        "                    cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "\n",
        "        gen_loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "\n",
        "    return disc_loss, gen_loss\n",
        "\n",
        "# Save generated frames after training\n",
        "def save_generated_frames(generator, num_samples, save_dir):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    noise = tf.random.normal([num_samples, latent_dim])\n",
        "    generated_videos = generator.predict(noise)\n",
        "\n",
        "    for i, video in enumerate(generated_videos):\n",
        "        video_path = os.path.join(save_dir, f\"generated_video_{i}\")\n",
        "        os.makedirs(video_path, exist_ok=True)\n",
        "        for j, frame in enumerate(video):\n",
        "            frame_path = os.path.join(video_path, f\"frame_{j}.npy\")\n",
        "            np.save(frame_path, frame)  # Save each frame as a .npy file\n",
        "        print(f\"Saved generated video {i} to {video_path}\")\n",
        "\n",
        "# Training Loop with frame-saving at the end\n",
        "def train(train_dataset, test_dataset, epochs, save_dir):\n",
        "    for epoch in range(epochs):\n",
        "        for video_batch in train_dataset:\n",
        "            disc_loss, gen_loss = train_step(video_batch)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Disc Loss: {disc_loss:.4f}, Gen Loss: {gen_loss:.4f}\")\n",
        "\n",
        "        # Evaluate generator on test set at each epoch\n",
        "        for test_video_batch in test_dataset:\n",
        "            test_generated = generator(tf.random.normal([batch_size, latent_dim]), training=False)\n",
        "            print(f\"Test Batch Generated Shape: {test_generated.shape}\")\n",
        "\n",
        "    # Save models after training\n",
        "    generator.save(os.path.join(save_path, \"asr_generator.keras\"))\n",
        "    discriminator.save(os.path.join(save_path, \"asr_discriminator.keras\"))\n",
        "    text_to_feature_mapping.save(os.path.join(save_path, \"final_text_to_feature_mapping.keras\"))\n",
        "\n",
        "    # Save generated frames to a folder\n",
        "    save_generated_frames(generator, num_samples=10, save_dir=os.path.join(save_path, \"generated_frames\"))\n",
        "\n",
        "# Start training\n",
        "train(train_dataset, test_dataset, epochs, save_dir=save_path)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-21T16:27:56.661921Z",
          "iopub.execute_input": "2024-11-21T16:27:56.662284Z",
          "iopub.status.idle": "2024-11-21T16:46:44.122433Z",
          "shell.execute_reply.started": "2024-11-21T16:27:56.662256Z",
          "shell.execute_reply": "2024-11-21T16:46:44.121504Z"
        },
        "id": "9ILEm8tUkBzV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# Directory containing the generated video folders with .npy files\n",
        "generated_frames_dir = \"/kaggle/working/generated_frames\"  # Adjust this path to where your frames are stored\n",
        "output_png_dir = \"/kaggle/working/color_color_png\"  # Output folder for the PNG files\n",
        "\n",
        "# Define the connections between keypoints for skeletal representation (only relevant body parts)\n",
        "connections = [\n",
        "    (0, 1), (1, 2), (2, 3), (3, 4),   # Thumb\n",
        "    (0, 5), (5, 6), (6, 7), (7, 8),   # Index\n",
        "    (0, 9), (9, 10), (10, 11), (11, 12),  # Middle\n",
        "    (0, 13), (13, 14), (14, 15), (15, 16),  # Ring\n",
        "    (0, 17), (17, 18), (18, 19), (19, 20),  # Pinky\n",
        "    (11, 12), (12, 13),  # Left arm\n",
        "    (14, 15), (15, 16),  # Right arm\n",
        "    (11, 14),  # Shoulders\n",
        "]\n",
        "\n",
        "# Function to draw skeletal gestures with colors and annotations\n",
        "def draw_colored_skeleton(image, keypoints, connections, colors, annotations, radius=5, thickness=2):\n",
        "    for idx, (start, end) in enumerate(connections):\n",
        "        if start < len(keypoints) and end < len(keypoints):\n",
        "            pt1 = tuple(keypoints[start][:2].astype(int))\n",
        "            pt2 = tuple(keypoints[end][:2].astype(int))\n",
        "            color = colors.get(idx, (255, 255, 255))  # Default to white if no color specified\n",
        "            cv2.line(image, pt1, pt2, color, thickness)\n",
        "\n",
        "    for idx, keypoint in enumerate(keypoints):\n",
        "        pt = tuple(keypoint[:2].astype(int))\n",
        "        cv2.circle(image, pt, radius, (0, 255, 0), -1)  # Default point color is green\n",
        "\n",
        "        # Annotate keypoints\n",
        "        if idx in annotations:\n",
        "            cv2.putText(\n",
        "                image, annotations[idx], pt, cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1, cv2.LINE_AA\n",
        "            )\n",
        "\n",
        "# Example of color mapping and annotations\n",
        "color_map = {\n",
        "    0: (255, 0, 0),  # Red for thumb\n",
        "    1: (0, 255, 0),  # Green for index finger\n",
        "    2: (0, 0, 255),  # Blue for middle finger\n",
        "    # Add more colors for other connections if needed\n",
        "}\n",
        "\n",
        "annotations = {\n",
        "    33: \"L-Shoulder\",\n",
        "    34: \"L-Elbow\",\n",
        "    35: \"L-Wrist\",\n",
        "    44: \"R-Shoulder\",\n",
        "    45: \"R-Elbow\",\n",
        "    46: \"R-Wrist\",\n",
        "    21: \"Right-Hand Start\",\n",
        "    42: \"Left-Hand Start\",\n",
        "}\n",
        "\n",
        "# Function to convert keypoints .npy files to gesture-like .png images with colors and annotations\n",
        "def npy_to_colored_gesture_png(input_dir, output_dir, image_size=(512, 512)):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Iterate through each video folder\n",
        "    for video_folder in os.listdir(input_dir):\n",
        "        video_path = os.path.join(input_dir, video_folder)\n",
        "        if os.path.isdir(video_path):\n",
        "            output_video_path = os.path.join(output_dir, video_folder)\n",
        "            os.makedirs(output_video_path, exist_ok=True)\n",
        "\n",
        "            # Iterate through all .npy files in the folder\n",
        "            for frame_file in sorted(os.listdir(video_path)):\n",
        "                if frame_file.endswith(\".npy\"):\n",
        "                    # Load keypoints from .npy file\n",
        "                    keypoints = np.load(os.path.join(video_path, frame_file))\n",
        "\n",
        "                    # Reshape keypoints for visualization (assuming it's flattened)\n",
        "                    keypoints = keypoints.reshape(-1, 3)  # Assumes (num_keypoints / 3, 3) structure\n",
        "\n",
        "                    # Filter out only the relevant keypoints (hands and pose)\n",
        "                    relevant_keypoints = np.concatenate([\n",
        "                        keypoints[21:42],  # Right Hand (21 keypoints)\n",
        "                        keypoints[42:63],  # Left Hand (21 keypoints)\n",
        "                        keypoints[33:34],  # Left Shoulder\n",
        "                        keypoints[34:35],  # Left Elbow\n",
        "                        keypoints[35:36],  # Left Wrist\n",
        "                        keypoints[44:45],  # Right Shoulder\n",
        "                        keypoints[45:46],  # Right Elbow\n",
        "                        keypoints[46:47],  # Right Wrist\n",
        "                    ])\n",
        "\n",
        "                    # Create a blank image\n",
        "                    image = np.zeros((image_size[0], image_size[1], 3), dtype=np.uint8)\n",
        "\n",
        "                    # Normalize keypoints to image coordinates\n",
        "                    relevant_keypoints[:, 0] = (relevant_keypoints[:, 0] + 1) * (image_size[1] / 2)  # X-axis\n",
        "                    relevant_keypoints[:, 1] = (relevant_keypoints[:, 1] + 1) * (image_size[0] / 2)  # Y-axis\n",
        "\n",
        "                    # Draw the skeleton with relevant keypoints\n",
        "                    draw_colored_skeleton(image, relevant_keypoints, connections, color_map, annotations)\n",
        "\n",
        "                    # Save as PNG\n",
        "                    frame_png_path = os.path.join(output_video_path, frame_file.replace(\".npy\", \".png\"))\n",
        "                    cv2.imwrite(frame_png_path, image)\n",
        "\n",
        "                    print(f\"Converted {frame_file} to {frame_png_path}\")\n",
        "\n",
        "# Convert .npy to gesture PNGs with colors and annotations\n",
        "npy_to_colored_gesture_png(generated_frames_dir, output_png_dir)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-21T17:27:54.749845Z",
          "iopub.execute_input": "2024-11-21T17:27:54.750225Z",
          "iopub.status.idle": "2024-11-21T17:27:55.921065Z",
          "shell.execute_reply.started": "2024-11-21T17:27:54.750191Z",
          "shell.execute_reply": "2024-11-21T17:27:55.920249Z"
        },
        "id": "zsc2mK03kBzW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "def zip_folder(folder_path, output_zip_path):\n",
        "    \"\"\"\n",
        "    Compress a folder into a zip file.\n",
        "\n",
        "    Args:\n",
        "        folder_path (str): Path to the folder to be compressed.\n",
        "        output_zip_path (str): Path to save the output .zip file (without the extension).\n",
        "    \"\"\"\n",
        "    # Make sure the output path does not have .zip at the end (shutil adds it automatically)\n",
        "    if output_zip_path.endswith(\".zip\"):\n",
        "        output_zip_path = output_zip_path[:-4]\n",
        "\n",
        "    # Compress the folder into a zip file\n",
        "    shutil.make_archive(output_zip_path, 'zip', folder_path)\n",
        "    print(f\"Folder '{folder_path}' compressed into '{output_zip_path}.zip'.\")\n",
        "\n",
        "# Example usage\n",
        "folder_to_compress = \"/kaggle/working/color_color_png\"  # Replace with your folder path\n",
        "output_zip_file = \"/kaggle/working/colorframes\"  # Replace with the desired zip file name (without extension)\n",
        "\n",
        "# Call the function\n",
        "zip_folder(folder_to_compress, output_zip_file)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-21T17:35:46.743026Z",
          "iopub.execute_input": "2024-11-21T17:35:46.743406Z",
          "iopub.status.idle": "2024-11-21T17:35:47.021074Z",
          "shell.execute_reply.started": "2024-11-21T17:35:46.743374Z",
          "shell.execute_reply": "2024-11-21T17:35:47.020378Z"
        },
        "id": "J_9VhrtzkBzX"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}